# Crypto_Diversity_Pipeline

Pipeline for genome assembly and analysis of data from Desjardins et al. 2017


## Broad description (new version)

Raw Illumina sequencing reads generated by Desjardins et al. 2017 were downloaded from the NIH Sequence Read Archive (BioProject ID PRJNA382844).  For each of the 387 BioSamples (corresponding strains of interest) associated with the BioProject, we created a reference based genome assembly based on aligning paired-end sequence data to the genome of the reference of the corresponding lineage (see below). In cases where there were multiple sequencing runs for a given BioSample, we concatenated the FASTQ files of the multiple runs into one.  

We used the following reference genome assemblies to align the reads of a sample of a given lineage to its corresponding reference:  
* VNI -- H99, GCA_011801205.1  
* VNII -- VNII strain collected from cockatoo excrement , GCA_022832995.1  
* VNBI -- Ftc146-1, GCA_010065285.1  
* VNBII -- Bt81, GCA_023650555.1  

To create reference based genome assemblies we aligned reads to the corresponding reference genome using BWA (vXXX; Li and Durbin 2009), called variants using FreeBayes (vXXX; Garrison and Marth 2012), and generated strain-specific consensus assembly by instantiating the called variants onto the corresponding reference genome.  The read alignment, variant calling, and consensus assembly were carried out using the Snippy (https://github.com/tseemann/snippy) pipeline tool.

To have a consistent naming scheme in the annotations we "lifted over" the annotation of the strain H99 (FungiDB R53) to each of the four reference genomes using the software tool Liftoff (vXXX; Shumate and Salzberg 2020). Following construction of consensus assemblies, genome feature annotation was "lifted over" from the corresponding reference genome to each strain-specific genome.  The `-polish` option of Liftoff was employed to re-align exons in cases where the lift-over procedure resulted in start/stop codon loss or introduced an in-frame stop codon.  

Based on the polished lift-over annotation, the AGAT GTF/GFF Toolkit software (https://github.com/NBISweden/AGAT) was used to predict protein sequences for all annotated genes in each strain-specific assembly using the `agat_sp_extract_sequences.pl` script. Where multiple protein isoforms are annotated in the reference genome, we generated predictions for each isoform.



## Requirements

* Python and Conda (miniconda is my preference)
* Xonsh -- https://xon.sh/
* Snakemake -- https://snakemake.github.io/
  * Graphviz -- https://graphviz.org/ (optional, to see Snakemake DAG in a graph)  
* NCBI Entrez Utilities (E-utilities) command line tools -- https://www.ncbi.nlm.nih.gov/books/NBK25501/
* NCBI SRA Tools -- https://github.com/ncbi/sra-tools
* Python modules -- Pandas
* R with tidyverse meta-package
* Snippy -- https://github.com/tseemann/snippy
* Liftoff -- https://github.com/agshumate/Liftoff
* AGAT -- https://github.com/NBISweden/AGAT
* Mosdepth -- https://github.com/brentp/mosdepth
* Samtools -- https://www.htslib.org/
  
### Installations  

Environment installation files are in `envs/`
<details>
<summary>Install crypto_div environment -- everything runs in this environment </summary>
 
With the `envs/crypto_div.yml` file:

~~~
nohup conda env create -y -f envs/crypto_div.yaml &
~~~

When the environment is ready install R:
~~~
conda activate crypto_div
conda install -c r r-essentials
conda deactivate
~~~
And install Graphviz to see Sankemake DAG of jobs in svg
~~~
conda install -c conda-forge graphviz
~~~

</details>

<details>
<summary> Install AGAT in its own environment -- when used in Snakemake the workflow uses a separate installation of the environment, but to use it independently install it with this instructions </summary>

Run this lines one by one:
~~~
 conda create -n agat
 conda activate agat
 conda install perl-bioperl perl-clone perl-graph perl-lwp-simple perl-carp perl-sort-naturally perl-file-share perl-file-sharedir-install perl-moose perl-yaml perl-lwp-protocol-https -c bioconda
 conda install r-base
 conda install perl-statistics-r -c bioconda
 cpan install bioperl List::MoreUtils Term::ProgressBar
 git clone https://github.com/NBISweden/AGAT.git
 perl Makefile.PL 
 make
 make test
 make install
 conda deactivate
 ~~~

</details>


## Overview

The working directory is `/analysis/czirion/Crypto_Diversity_Pipeline/`

Scripts to be run in this order:

1. `CryptoDiversity-Retrieve.xsh` -- given an NCBI BioProject ID, identifies all the BioSamples associated with that project and downloads each into a folder called `Samples/${SRSID}` where `SRSID` are SRA SRS numbers
  
    * Files produced:
      
      * "{project_id}_samples.txt" -- a tab-delimited text file listing all the samples associated with the BioProject.  Three columns: SAM, Name, SRS  where SAM = BioSample Sample ID , Name = common name, SRS = SRA sequence read identifier
      
      * "{project_id}_SRStoSRR.txt" -- tab-delimited text file giving mapping between SRS (Samples) IDs and SRR (Runs) IDs. A given SRS can have multiple associated SRRs. 
      
      * Each `Samples/${SRSID}` directory will contain a number of SRR subdirectories of the form `Samples/${SRSID}/{SRRID}` where SRA runs are "prefetched" using the SRA Tools `prefetch` command.


2. `fastq-unpacker.xsh` -- turn each of the "prefetched" run files (produced in the prior step) into FASTQ files, written to `./fastqs` directory

3. ⚠️ FIXME `build-read-pair-tables.xsh` -- create CSV files for paired and unpaired fastqs
   * Files produced:
  
     * `read_pair_table.csv` -- Columns are SRS ID, SRR ID, read pair 1, read pair 2, total size in bytes of read pair

     * `largest_read_pair_table.csv` -- A filtered version of `read_pair_table.csv`, giving the single largest pair per sample
  
     * `unpaired_fastqs.csv` -- Columns are SRS ID, SRR ID, fastq file(s)

4. `get-lineage-of-samples.xsh` -- add the SRS codes to the `Desjardins_Supplemental_Table_S1.csv`
   * Files produced:
  
     * `sample_reference.csv`  
     * `sample_metadata.csv`  

5. `get-references.xsh` -- uses `lineage_references.csv` and `sample_metadata.csv` to create a table with each sample name, fastq filenames, lineage, and corresponding reference assembly filename
   * Files produced:
  
     * `sample_reference.csv`  

6. `get-chromosome-names.sh` -- uses `lineage_references.csv` and the FASTA files of the reference genomes, to generate a table with the correspondance between the sequence ID of each chromosome and the common chromosome number of lineage.  
   * Files produced:
  
     * `chromosome_names.csv` 

7. `Snakefile-references.smk` -- is a Snakefile to lift over annotations from `FungiDB-53_CneoformansH99_PMM.gff` into the four lineages genomes. It currently works with:  
  ` snakemake --snakefile Snakefile-references.smk --cores 1 --use-conda --conda-frontend conda -p`:  
      ⚠️ `--cores 1` is because there is a problem if Liftoff runs in parallel because the different jobs try to create `FungiDB-53_CneoformansH99_PMM.gff_db` at the same time and that is not cool.    
      ⚠️ `--conda-frontend conda` because it cannot use mamba, which is the default.  
      ❔  It makes `{lineage}_liftoff.agat.log` files out of nowhere, they are not specified in the snakefile.  
      ⏰ Pending: Merge into main workflow.
   * Files produced:
  
     * 

8. `Snakefile-main.smk`: Is the Snakefile to run the pipeline, it uses the `config.yaml` file.   For the moment it does the following:  
  * Runs the script `scripts/fastq-combiner.xsh` for each sample in `read_pair_table.csv`. This concatenates all `_1.fastq` of one sample into only one file named `<SRS-accession>_1.fq.gz` and compresses it and does the same for `_2.fastq`.  This files are in `fastq_combined/`.
  * Runs **snippy** for each sample.  
  * Runs **liftoff** for each sample.
  * Runs **agat** for each sample.
  * Makes **fasta indexes** for the protein and cds fastas.
  * **Extracts sequences** (cds and protein) of each sample and puts it in a file.  
  * **Concatenate** all sequences for each protein (cds and protein) in only one file.  

    * Files produced:  
    
      * 

9. `Snakefile-depth-quality.smk`: Generates **quality and coverage** plots.  
   * Files produced:  
  
     * 
